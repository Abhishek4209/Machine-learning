{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment:- KNN-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q1:-**  \n",
    "### **What is the KNN algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q2:-** \n",
    "### **How do you choose the value of K in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q3:-** \n",
    "### **What is the difference between KNN classifier and KNN regressor?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Knn Classifier: Predicts a class by using the highest majority category among its k nearest neighbors. Knn Regression: Predicts a value by using the mean of the k nearest neighbors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q4:-** \n",
    "### **How do you measure the performance of KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This classification is based on measuring the distances between the test sample and the training samples to determine the final classification output. The traditional k-NN classifier works naturally with numerical data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q5:-** \n",
    "### **What is the curse of dimensionality in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q6:-** \n",
    "### **How do you handle missing values in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q7:-** \n",
    "### **Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q8:-** \n",
    "### **What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Advantages of KNN Algorithm:-**\n",
    "### **1.Simple and Easy to Understand:–** \n",
    "#### **The KNN algorithm is simple and easy to understand, making it a popular choice for beginners in the field of machine learning. The basic idea of the algorithm is to find the K nearest data points to a given test data point and use the majority class among them to classify the test data point.**\n",
    "### **2.Non-parametric:-**  \n",
    "#### **The KNN algorithm is a non-parametric algorithm, meaning that it does not make any assumptions about the underlying distribution of the data. This makes it a flexible algorithm that can be used in a wide range of applications.**\n",
    "### **3.No Training Required:-**  \n",
    "#### **The KNN algorithm does not require any training process, which means that it can be used in real-time applications where data is continuously being generated.**\n",
    "### **4.Can Handle Large Datasets:-** \n",
    "#### **The KNN algorithm can handle large datasets without suffering from the curse of dimensionality, which is a common problem in other machine learning algorithms. This makes it a suitable algorithm for problems with high-dimensional data.**\n",
    "### **5.Accurate and Effective:-** \n",
    "#### **The KNN algorithm is known for its accuracy and effectiveness, particularly when used with small to medium-sized datasets. It is a robust algorithm that can handle noisy and incomplete data, making it a popular choice in many real-world applications.**\n",
    "## **Disadvantages of KNN Algorithm:-**\n",
    "### **1.Sensitive to Outliers:-**  \n",
    "#### **The KNN algorithm can be sensitive to outliers in the data, which can significantly affect its performance. Outliers are data points that are significantly different from the rest of the data, and they can have a disproportionate impact on the KNN algorithm’s classification results.**\n",
    "### **2.Computationally Expensive:-**  \n",
    "#### **The KNN algorithm can be computationally expensive, particularly for large datasets. This is because the algorithm needs to compute the distance between each test data point and every training data point, which can be time-consuming.**\n",
    "### **3.Requires Good Choice of K:-**  \n",
    "#### **The KNN algorithm requires a good choice of the K parameter, which determines the number of nearest neighbors used for classification. If K is too small, the algorithm may be too sensitive to noise in the data, while if K is too large, the algorithm may miss important patterns in the data.**\n",
    "### **4.Limited to Euclidean Distance:-**  \n",
    "#### **The KNN algorithm is limited to using the Euclidean distance metric to measure the distance between data points. This can be a disadvantage when working with non-Euclidean data, such as categorical or binary data.**\n",
    "### **5.Imbalanced Data:-**  \n",
    "#### **The KNN algorithm can struggle with imbalanced data, where one class has significantly more data points than the other. This can lead to biased classification results, where the majority class is always predicted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q9:-** \n",
    "### **What is the difference between Euclidean distance and Manhattan distance in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Manhattan distance captures the distance between two points by aggregating the pairwise absolute difference between each variable while Euclidean distance captures the same by aggregating the squared difference in each variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q10:-**  \n",
    "### **What is the role of feature scaling in KNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scaled, the feature with a higher value range starts dominating when calculating distances. KNN which uses Euclidean distance is one such algorithm which essentially require scaling**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
