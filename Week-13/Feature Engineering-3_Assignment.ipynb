{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q1:-**  \n",
    "### **What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Min-Max scaling, also known as Min-Max normalization or feature scaling, is a data preprocessing technique used in machine learning and statistics to scale numerical features within a specific range, typically between 0 and 1. The goal of Min-Max scaling is to transform the original feature values in such a way that they all fall within a uniform range, making them suitable for models that are sensitive to the magnitude of the features, such as neural networks, support vector machines, and k-nearest neighbors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The formula for Min-Max scaling of a feature is as follows**\n",
    "#### **Xscaled=(X-Xmin)/(Xmax-Xmin)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **X is the original feature value.**\n",
    "#### **Xscaled is the scaled feature value.**\n",
    "#### **X min is the minimum value of the feature in the dataset.**\n",
    "#### **X max is the maximum value of the feature in the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q2:-**\n",
    "### **What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q3:-** \n",
    "### **What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principal Component Analysis (PCA) is a dimensionality reduction technique used in the fields of machine learning, statistics, and data analysis. Its primary purpose is to reduce the number of features (dimensions) in a dataset while preserving as much of the essential information as possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Suppose you have a dataset with two features, \"Height\" (in inches) and \"Weight\" (in pounds), for a group of individuals. You want to reduce the dimensionality of this dataset using PCA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.Data Standardization: Standardize the \"Height\" and \"Weight\" features to have a mean of 0 and a standard deviation of 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Covariance Matrix: Calculate the covariance matrix of the standardized data. It will be a 2x2 matrix that describes how \"Height\" and \"Weight\" vary together.**\n",
    "#### **Eigenvalue Decomposition: Compute the eigenvectors and eigenvalues of the covariance matrix. Let's say the first principal component (PC1) corresponds to the eigenvector [0.8, 0.6] and has an eigenvalue of 1.5, and the second principal component (PC2) corresponds to the eigenvector [-0.6, 0.8] and has an eigenvalue of 0.5.**\n",
    "#### **Sorting: The first principal component (PC1) has the largest eigenvalue, indicating that it explains more variance in the data.**\n",
    "#### **Selecting Principal Components: You decide to retain only one principal component, PC1, which explains most of the variance.**\n",
    "#### **Dimensionality Reduction: Project the original data onto PC1 to obtain a new one-dimensional dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q4:-**  \n",
    "### **What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principal component analysis (PCA) is an unsupervised linear transformation technique which is primarily used for feature extraction and dimensionality reduction**\n",
    "### **Principal Component Analysis(PCA) is feature extraction technique meant to reduce the dimensions of our dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q5:-** \n",
    "### **You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using Min-Max scaling to preprocess data for building a recommendation system for a food delivery service can be helpful to ensure that the features have consistent scales and do not dominate the recommendation process. Min-Max scaling transforms the features into a specified range, typically [0, 1], making them more suitable for various machine learning algorithms, including recommendation systems. Here's how you would use Min-Max scaling in this context:**\n",
    "#### **1.Data Collection and Understanding:**\n",
    "#### **2.Data Preprocessing:**\n",
    "#### **3.Min-Max Scaling:**\n",
    "#### **4.Implementation:**\n",
    "#### **5.Scaled Dataset:**\n",
    "#### **6.Model Building:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q6:-**  \n",
    "### **You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for building a stock price prediction model can be a valuable preprocessing step. PCA helps in simplifying the dataset, removing noise or redundancy, and often improves the efficiency and performance of machine learning models. Here's how you would use PCA for dimensionality reduction in this context**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.Data Preprocessing:**\n",
    "#### **2.Standardization:**\n",
    "#### **3.PCA Implementation:**\n",
    "#### **4.Explained Variance:**\n",
    "#### **5.Dimensionality Reduction:**\n",
    "#### **6.Model Building:**\n",
    "#### **7.Evaluation and Fine-Tuning:**\n",
    "#### **8.Interpretation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q7:-**  \n",
    "### **For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Find the minimum and maximum values in the dataset:**\n",
    "#### Minimum (X_min) = 1\n",
    "#### Maximum (X_max) = 20**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Apply the Min-Max scaling formula to each data point:**\n",
    "#### For 1:\n",
    "#### Xscaled=(1-1)/(20-1) x(1-(-1))+(-1)=0\n",
    "#### For 5:\n",
    "#### Xscaled=(5-1)/(20-1) x(1-(-1))+(-1)=-0.5\n",
    "#### For 10:\n",
    "#### Xscaled=(10-1)/(20-1) x(1-(-1))+(-1)=-0.1\n",
    "#### For 15:\n",
    "#### Xscaled=(15-1)/(20-1) x(1-(-1))+(-1)=0.3\n",
    "#### For 20:\n",
    "#### Xscaled=(20-1)/(20-1) x(1-(-1))+(-1)=1\n",
    "#### Now, the dataset [1, 5, 10, 15, 20] has been Min-Max scaled to the range [-1, 1]:\n",
    "\n",
    "#### [−1,−0.5,−0.1,0.3,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q8:-**  \n",
    "### **For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional one while preserving as much variance as possible. When applying PCA to a dataset with features like the ones you've mentioned (height, weight, age, gender, blood pressure), the goal is to reduce the dimensionality while retaining as much meaningful information as possible. To determine how many principal components to retain, you typically follow these steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.Standardize the data: Before applying PCA, it's essential to standardize the data by subtracting the mean and scaling to unit variance. This step ensures that features with larger scales (e.g., weight and blood pressure) do not dominate the PCA process.**\n",
    "### **2.Compute the covariance matrix: Calculate the covariance matrix of the standardized data. This matrix represents the relationships between different features and is essential for PCA.**\n",
    "### **3.Calculate eigenvectors and eigenvalues: The eigenvectors and eigenvalues of the covariance matrix are computed. Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each principal component.**\n",
    "### **4.Sort eigenvalues: Sort the eigenvalues in descending order. The eigenvalues represent the amount of variance explained by each principal component. Higher eigenvalues indicate more variance.**\n",
    "### **5.Decide on the number of components to retain: There is no fixed rule for choosing the number of principal components to retain, but a common approach is to set a cumulative explained variance threshold. You sum the eigenvalues until you reach a desired level of explained variance (e.g., 95% or 99%). The number of principal components needed to achieve this threshold is the number you retain.**\n",
    "### **6.Transform the data: Finally, you transform the original data into the new lower-dimensional space using the retained principal components.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
