{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q1:-** \n",
    "### **What is a contingency matrix, and how is it used to evaluate the performance of a classification model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A contingency matrix, also known as a confusion matrix, is a table used in the field of machine learning and statistics to evaluate the performance of a classification model. It helps in assessing how well a classification algorithm is performing by comparing its predictions to the actual true values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.True Positives (TP):These are the cases where the model correctly predicted the positive class (e.g., correctly identifying a disease when the patient has it).**\n",
    "##### **2.True Negatives (TN): These are the cases where the model correctly predicted the negative class (e.g., correctly identifying a healthy patient as not having a disease).**\n",
    "##### **3.False Positives (FP): These are the cases where the model incorrectly predicted the positive class when the true class was negative (e.g., incorrectly diagnosing a healthy patient as having a disease). False positives are also known as Type I errors.**\n",
    "\n",
    "##### **4.False Negatives (FN): These are the cases where the model incorrectly predicted the negative class when the true class was positive (e.g., failing to diagnose a patient with a disease when they actually have it). False negatives are also known as Type II errors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Accuracy: Accuracy measures the overall correctness of the model's predictions and is calculated as**\n",
    "**(TP + TN) / (TP + TN + FP + FN).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q2:-**  \n",
    "### **How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pairwise Confusion Matrix:-**\n",
    "#### **In some multi-class classification scenarios, you might be interested in evaluating the performance of the model specifically for pairs of classes. Instead of considering all classes at once, you can create confusion matrices for each pair of classes. This can be useful in situations where you want to understand how well the model distinguishes between specific classes while ignoring the rest.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q3:-**   \n",
    "### **What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extrinsic evaluation is also called task-based evaluation and captures how useful the model is in a particular task that is used in downstream applications. Entropy, Cross entropy, and Perplexity are common metrics for evaluating the performance of language models in NLP.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q4:-**  \n",
    "### **What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In intrinsic evaluation, system output is evaluated against the pre-determined ground truth (reference text) whereas in extrinsic evaluation quality of system output is assessed based on its impact on the performance of other NLP systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q5:-**  \n",
    "### **What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A confusion matrix is a table that summarizes the performance of a machine learning classifier by comparing its predicted and actual labels. It is a useful tool to evaluate how well your model can distinguish between different classes and identify its strengths and weaknesses.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q6**  \n",
    "### **What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The evaluation metrics which do not require any ground truth labels to calculate the efficiency of the clustering algorithm could be used for the computation of the performance evaluation. There are three commonly used evaluation metrics: Silhouette score, Calinski Harabaz index, Davies-Bouldin Index.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q7:-**  \n",
    "### **What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ans:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1: Imbalanced Classes. Many categorization issues in the actual world have unequal distributions of the classes.**\n",
    "### **2: Misclassification Costs.**\n",
    "### **3: Probability Predictions.**\n",
    "### **4: Ambiguity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **in the framework of imbalanced data-sets, accuracy is no longer a proper measure, since it does not distinguish between the numbers of correctly classified examples of different classes. Hence, it may lead to erroneous conclusions.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
